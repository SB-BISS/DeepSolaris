{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'roofs/'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-555c283903e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#load everything from images (quite slow) can be skipped, reading features from csv, block 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mkpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sift_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roofs/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded Positives'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mkpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkpoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sift_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no_roofs/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/Solar_Low_Res/feature_extraction.py\u001b[0m in \u001b[0;36mget_sift_descriptors\u001b[0;34m(folder_name, step, ws)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mkp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'roofs/'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import feature_extraction as fe\n",
    "\n",
    "#load everything from images (quite slow) can be skipped, reading features from csv, block 2\n",
    "kpoints = fe.get_sift_descriptors('roofs/', step=4, ws=8)\n",
    "print ('Loaded Positives', kpoints.shape)\n",
    "kpoints = np.concatenate((kpoints,fe.get_sift_descriptors('no_roofs/', step=4, ws=8)))\n",
    "print ('Loaded Negatives', kpoints.shape)\n",
    "\n",
    "print (kpoints.shape)\n",
    "\n",
    "feat_names = ['F'+str(i) for i in range(128)]\n",
    "data = pd.DataFrame(kpoints,columns=feat_names)\n",
    "data.to_csv('features.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data loaded from CSV', (452064, 128))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feature_extraction as fe\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('features.csv')\n",
    "data_arr = np.delete(data.values, 0, axis=1)\n",
    "'Data loaded from CSV',data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_arr' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b0b33dec93d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'200_words.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m'Model Dumped to File'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_arr' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "#slow training.use pickled file, next box\n",
    "k_words = 200\n",
    "kmeans = KMeans(k_words, max_iter=300, verbose=1, precompute_distances=True, n_init=2, n_jobs=2)\n",
    "kmeans.fit(data_arr)\n",
    "joblib.dump(kmeans, '200_words.pkl')\n",
    "'Model Dumped to File'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francesco/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator KMeans from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n    n_clusters=100, n_init=2, n_jobs=2, precompute_distances=True,\n    random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "kmeans = joblib.load('50_words.pkl') \n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded roofs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(961, 100) (961,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import feature_extraction as fe\n",
    "import os\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "#slow feature extraction problem, load txt files from next box\n",
    "\n",
    "print ('Starting to load features')\n",
    "for file in os.listdir('roofs/'):\n",
    "    if file.endswith('png'):\n",
    "        img = cv2.imread('roofs/'+file)\n",
    "        X.append(fe.get_features(img,kmeans))\n",
    "        Y.append(1)\n",
    "        \n",
    "print ('Loaded roofs')      \n",
    "for file in os.listdir('no_roofs/'):\n",
    "    if file.endswith('png'):\n",
    "        img = cv2.imread('no_roofs/'+file)\n",
    "        X.append(fe.get_features(img,kmeans))\n",
    "        Y.append(0)\n",
    "        \n",
    "\n",
    "X = normalize(np.array(X), norm='max')\n",
    "Y = np.array(Y).reshape(-1,)\n",
    "\n",
    "np.savetxt('x50.txt', X, delimiter=',')\n",
    "np.savetxt('y50.txt', Y, delimiter=',')\n",
    "\n",
    "print (X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.loadtxt('x50.txt', delimiter=',')\n",
    "Y = np.loadtxt('y50.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] KNN CV_Score: 0.8010429789353412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM1 CV_Score: 0.5452657415274237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM2 CV_Score: 0.6045208774825275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RF CV_Score: 0.8372900777025817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NN CV_Score: 0.799972004934944\n[INFO] GNB CV_Score: 0.7031513979628707\n[INFO] Best CV_Score Achieved: 0.8372900777025817 by RF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=100, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "utils.classifiers_cv_score(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n        0.0       0.81      0.88      0.84        80\n        1.0       0.91      0.86      0.88       113\n\navg / total       0.87      0.87      0.87       193\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=3)\n",
    "model = MLPClassifier(alpha=1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = np.around(model.predict(X_test))\n",
    "print (classification_report(Y_test,Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n        0.0       0.77      0.88      0.82        80\n        1.0       0.90      0.81      0.86       113\n\navg / total       0.85      0.84      0.84       193\n\n"
     ]
    }
   ],
   "source": [
    "from utils import simple_ann\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "model = simple_ann(100, 1)\n",
    "model.compile(loss=['binary_crossentropy'], optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train, epochs=400,batch_size=200,validation_split=0.1, verbose=0)\n",
    "\n",
    "Y_pred = np.around(model.predict(X_test))\n",
    "print (classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n        0.0       0.79      0.88      0.83        80\n        1.0       0.90      0.83      0.87       113\n\navg / total       0.86      0.85      0.85       193\n\n"
     ]
    }
   ],
   "source": [
    "#model = RandomForestClassifier(max_depth=100, n_estimators=1000, max_features='auto')\n",
    "model = MLPClassifier(alpha=1)\n",
    "#model =KNeighborsClassifier(3)\n",
    "model.fit(X,Y)\n",
    "\n",
    "img = cv2.imread('test2.png')\n",
    "sq = 40\n",
    "ind = (len(img) // sq)\n",
    "sub = []\n",
    "\n",
    "for i in range(sq):\n",
    "    for j in range(sq):\n",
    "        sub.append(img[i*ind:(i + 1) * ind, j*ind:(j + 1) * ind])\n",
    "\n",
    "roofs = 0\n",
    "for i in sub:\n",
    "    if model.predict(fe.get_features(i,kmeans).reshape(1,-1)) == 1:\n",
    "        roofs += 1\n",
    "        cv2.imwrite('eval/r'+str(roofs)+'.png', i)\n",
    "        \n",
    "roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
